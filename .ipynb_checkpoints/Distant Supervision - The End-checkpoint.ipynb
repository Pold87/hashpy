{
 "metadata": {
  "name": "",
  "signature": "sha256:ec74c533cc6a645eae74bd1d8fda0ead4315ac2fb9c6ac1129120835aaf694ce"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Import"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# General\n",
      "import os, csv, time\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "# Twitter API\n",
      "import tweepy\n",
      "from tweepy.streaming import StreamListener\n",
      "from tweepy import Stream\n",
      "\n",
      "# unicode tokenizer\n",
      "import ucto\n",
      "\n",
      "# scikit-learn for machine learning algortihms\n",
      "from sklearn.base import BaseEstimator\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import metrics\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.pipeline import FeatureUnion\n",
      "from sklearn.cross_validation import LeaveOneOut\n",
      "from sklearn import cross_validation\n",
      "\n",
      "from sklearn.feature_selection import chi2\n",
      "\n",
      "from random import shuffle\n",
      "\n",
      "# Saving and Reading Python objects\n",
      "import pickle \n",
      "\n",
      "# import logging # not sure"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 252
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Settings"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.chdir(\"/home/pold/Dropbox/Uni/Radboud/Text_Mining/Endterm/hashpy\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 253
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "General Classifier"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Read Aggressive Tweets from TSV files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_key(x, path = \"tweets_better/aggressive_quest\"):\n",
      "    '''Create a file name from a number'''\n",
      "    return os.path.join(path, \"fold_\" + str(x) + \".txt\")\n",
      "\n",
      "# Create a list of file names of the manually labeled tweets.\n",
      "files = [create_key(x) for x in xrange(0,10)]\n",
      "\n",
      "# Store everything in a pandas data frame.\n",
      "list = []\n",
      "for file in files:\n",
      "    df = pd.read_table(file.encode('utf-8'),\n",
      "                       names = ('category', 'user', 'date', 'time', 'message'), \n",
      "                       header = None,\n",
      "                       index_col = None)\n",
      "    list.append(df)\n",
      "        \n",
      "frame = pd.concat(list, axis=0, keys = files)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 254
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Use Ucto for Dutch tokenization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def remove_non_ascii(s):\n",
      "    return \"\".join(i for i in s if ord(i) < 128)\n",
      "\n",
      "# Ucto settings\n",
      "settingsfile = \"/etc/ucto/tokconfig-nl-twitter\"\n",
      "tokenizer = ucto.Tokenizer(settingsfile)\n",
      "\n",
      "def ucto_tokenizer(str):\n",
      "    '''Tokenize string'''\n",
      "    remove_non_ascii(str)\n",
      "    tokenizer.process(unicode(str))\n",
      "    tokens = [unicode(token).encode('utf-8') for token in tokenizer]\n",
      "    for pos, token in enumerate(tokens): \n",
      "        # Create dummy values for users and urls.\n",
      "        if token.startswith('http:'): tokens[pos] = 'url'\n",
      "        if token.startswith('@'): tokens[pos] = 'user'\n",
      "    return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 255
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Aggressive Hashtags - Wordlist"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# List a selected hashtags\n",
      "with open(\"wordlists/wordlist_selection.txt\") as f:\n",
      "    selected_hashtags = f.read().splitlines()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 256
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Twitter"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Authentication"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "consumer_key = 'zdprSqld7JxkSjaeqQppemW5y'\n",
      "consumer_secret = ''\n",
      "access_token = '1612711080-64a9w2e4BOTV4id4HTqLpzaO8Z6ag0pNVE9axZv'\n",
      "access_token_secret = ''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Connect to Twitter"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
      "auth.set_access_token(access_token, access_token_secret)\n",
      "api = tweepy.API(auth)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 258
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Create a Query"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_query(wordlist):\n",
      "    '''Create a string for an OR query on Twitter'''\n",
      "    return \" OR \".join(wordlist)\n",
      "\n",
      "def create_negate_query(wordlist):\n",
      "    '''Create a string for a negated query on Twitter'''\n",
      "    return \" \".join([\"-\" + word for word in wordlist])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 236
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Collect Tweets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results_aggressive = []\n",
      "results_non_aggressive = []\n",
      "\n",
      "# Create queries\n",
      "query_pos = create_query(selected_hashtags)\n",
      "query_neg = create_negate_query(selected_hashtags)\n",
      "\n",
      "# Connect to Twitter and collect aggressive tweets\n",
      "for tweet in tweepy.Cursor(api.search, q = query_pos,  lang = \"nl\").items(1500):\n",
      "    results_aggressive.append(tweet)\n",
      "    \n",
      "# Wait some time to get not rate limited    \n",
      "time.sleep(60)\n",
      "\n",
      "# Same for non-aggressive Tweets (equal amount to aggressive tweets)\n",
      "for tweet in tweepy.Cursor(api.search\n",
      "                         , q = query_neg\n",
      "                         ,  lang = \"nl\").items(len(results_aggressive)):\n",
      "    results_non_aggressive.append(tweet)   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Count Hashtags"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hash_dict = { }\n",
      "\n",
      "# Initialize dictionary\n",
      "for word in selected_hashtags:\n",
      "    hash_dict[word] = 0\n",
      "\n",
      "# Count occurances of hashtags    \n",
      "for tweet in results_aggressive:\n",
      "    for word in selected_hashtags:\n",
      "        if word in tweet.text: hash_dict[word] += 1\n",
      "\n",
      "# Print collected same stats about collected tweets            \n",
      "#print(\"Amount of aggressive tweets:\", len(results_aggressive))\n",
      "#print(\"Amount of non-aggressive tweets:\", len(results_non_aggressive))\n",
      "#print(\"Hashtags frequencies:\")\n",
      "#print(hash_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 259
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Additional Feature Extractor (Linguistic Markers)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Transformer for extracting linguistic features in the message        \n",
      "# Inspired by (and uses code of) Andreas Mueller:\n",
      "# https://github.com/amueller/kaggle_insults/\n",
      "class LinguisticMarkers(BaseEstimator):\n",
      "\n",
      "    def fit(self, documents, y=None):\n",
      "        return self\n",
      "\n",
      "    def transform(self, documents):\n",
      "        \n",
      "        n_words = [len(ucto_tokenizer(remove_non_ascii(tweet))) for tweet in documents]\n",
      "        n_chars = [len(tweet) for tweet in documents]\n",
      "        \n",
      "        # number of uppercase words\n",
      "        allcaps = [np.sum([w.isupper() for w in comment.split()])\n",
      "               for comment in documents]\n",
      "        \n",
      "        allcaps_ratio = (np.array(allcaps) / np.array(n_chars, dtype=np.float)) \n",
      "        \n",
      "        # Total number of exlamation marks\n",
      "        exclamation = [c.count(\"!\") for c in documents]\n",
      "        \n",
      "        exclamation_ratio = (np.array(exclamation) / np.array(n_chars, dtype=np.float))\n",
      "        \n",
      "        # Total number of addressed users\n",
      "        users = [c.count(\"@\") for c in documents]\n",
      "\n",
      "        return np.array([allcaps\n",
      "                      , allcaps_ratio\n",
      "                      , exclamation\n",
      "                      , exclamation_ratio\n",
      "                      , users\n",
      "]).T "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 260
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Additional Features (Swear Words)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Transformer for extracting swear words in the message        \n",
      "# Inspired by (and uses code of) Andreas Mueller:\n",
      "# https://github.com/amueller/kaggle_insults/\n",
      "class SwearWords(BaseEstimator):\n",
      "    \n",
      "    def __init__(self):\n",
      "        with open(\"wordlists/swearwords-nl.txt\") as f:\n",
      "            swearwords = [l.strip() for l in f.readlines()]\n",
      "        self.swearwords_ = swearwords\n",
      "\n",
      "    def fit(self, documents, y=None):\n",
      "        return self\n",
      "\n",
      "    def transform(self, documents):\n",
      "        \n",
      "        # number of words\n",
      "        n_words = [len(ucto_tokenizer(remove_non_ascii(tweet))) for tweet in documents]\n",
      "        \n",
      "        # number of swear words:\n",
      "        n_swearwords = [np.sum([c.lower().count(w) for w in self.swearwords_])\n",
      "                                                for c in documents]\n",
      "        \n",
      "        swearwords_ratio = np.array(n_swearwords) / np.array(n_words, dtype=np.float)\n",
      "        \n",
      "        return np.array([n_swearwords\n",
      "                         , swearwords_ratio]).T "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 261
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "k-Fold Cross Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def leave_one_out_evaluation(dataset\n",
      "                           , testset = None\n",
      "                           , pipe = 0\n",
      "                           , add_selection_to_stopwords = False):\n",
      "    \n",
      "    if add_selection_to_stopwords:\n",
      "        stopwords_path_to_file = \"wordlists/dutch-stop-words-added.txt\"\n",
      "    else:\n",
      "        stopwords_path_to_file = \"wordlists/dutch-stop-words.txt\" \n",
      "        \n",
      "    with open(stopwords_path_to_file) as f:\n",
      "        dutch_stop_words = f.read().splitlines()\n",
      "\n",
      "    # Feature Extractors    \n",
      "    count_vect = CountVectorizer(stop_words = dutch_stop_words\n",
      "                                 , tokenizer = ucto_tokenizer\n",
      "                                 , lowercase = True)\n",
      "\n",
      "    linguisticmarkers = LinguisticMarkers()\n",
      "    swearwords = SwearWords()\n",
      "\n",
      "    # Combined features\n",
      "    combined0 = count_vect\n",
      "\n",
      "    combined1 = FeatureUnion([(\"feat1\", count_vect)\n",
      "                            , (\"feat2\", linguisticmarkers)])\n",
      "    \n",
      "    combined2 = FeatureUnion([(\"feat1\", count_vect)\n",
      "                            , (\"feat3\", swearwords)])    \n",
      "\n",
      "    combined3 = FeatureUnion([(\"feat1\", count_vect)\n",
      "                            , (\"feat2\", linguisticmarkers)\n",
      "                            , (\"feat3\", swearwords)])\n",
      "    \n",
      "    \n",
      "    \n",
      "    combined4 = linguisticmarkers\n",
      "    combined5 = FeatureUnion([(\"feat2\", linguisticmarkers)\n",
      "                            , (\"feat3\", swearwords)])   \n",
      "    \n",
      "    combined6 = swearwords       \n",
      "\n",
      "    \n",
      "    pipe_list = [combined0, combined1, combined2\n",
      "               , combined3, combined4, combined5, combined6]\n",
      "\n",
      "    tfidf = TfidfTransformer(sublinear_tf = True\n",
      "                           , use_idf = True)    \n",
      "\n",
      "\n",
      "    # Create classifier\n",
      "    clf = LogisticRegression(penalty = 'l2', dual = False) \n",
      "\n",
      "\n",
      "    # Create pipelines\n",
      "    pipeline = Pipeline([('vect', pipe_list[pipe]),\n",
      "                         ('tfidf', tfidf),\n",
      "                         ('clf', clf)])\n",
      "    \n",
      "    # Create collectors\n",
      "    predicted_per_fold = []\n",
      "    target_per_fold = pd.Series()\n",
      "\n",
      "    # If no test set is given, train the classifier do a k-fold cross-validation.\n",
      "    if testset is None:\n",
      "    \n",
      "        # Create random test and fold sets\n",
      "        kf = cross_validation.KFold(n=len(dataset), n_folds=10, shuffle=True)\n",
      "\n",
      "        for train_num, test_num in kf:\n",
      "\n",
      "            messages_train = dataset['message'][train_num]\n",
      "            categories_train = dataset['category'][train_num]\n",
      "\n",
      "            messages_test = dataset['message'][test_num]\n",
      "            categories_test = dataset['category'][test_num]         \n",
      "\n",
      "\n",
      "            target_per_fold = pd.concat([target_per_fold, categories_test])\n",
      "\n",
      "            # train classifier \n",
      "\n",
      "            # Remember to chose combined 1, 2 or 3 in Pipeline\n",
      "            pipeline = pipeline.fit(messages_train, categories_train)\n",
      "\n",
      "            predicted = pipeline.predict(messages_test)\n",
      "            predicted_per_fold.append(predicted)\n",
      "\n",
      "        # flatten predicted_per_fold\n",
      "        predicted_flattened = [item for sublist in predicted_per_fold for item in sublist]\n",
      "        return (np.array(predicted_flattened), target_per_fold)\n",
      "    \n",
      "    \n",
      "    # If a test set is available, train the classifier on the dataset and \n",
      "    # evaluate it on the test set\n",
      "    else:                              \n",
      "        \n",
      "        # Remember to chose combined 1, 2 or 3 in Pipeline\n",
      "        pipeline = pipeline.fit(dataset['message'], dataset['category'])\n",
      "        \n",
      "        predicted = pipeline.predict(testset['message'])\n",
      "        \n",
      "        return (predicted, testset['category'])\n",
      "        \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 262
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Evaluation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def labels2ints(ls):\n",
      "    '''Convert aggressive to 1 and non-aggressive to 0'''\n",
      "    out = []\n",
      "    for label in ls:\n",
      "        if label == 'aggressive': out.append(1)\n",
      "        else: out.append(0) \n",
      "    return out\n",
      "\n",
      "def evaluation(tpf, ppf):\n",
      "    ''' Perform the complete evaluation'''\n",
      "    predicted_per_fold_array = np.array(ppf)\n",
      "\n",
      "    target_int = labels2ints(tpf)\n",
      "    predicted_int = labels2ints(predicted_per_fold_array)\n",
      "    \n",
      "    #print(\"Accuracy\")\n",
      "    #print(metrics.accuracy_score(target_int, predicted_int))  \n",
      "    \n",
      "    #print(\"Classic Report\")\n",
      "    #print(metrics.classification_report(target_int, predicted_int))  \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 269
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Run Evaluation of manually labeled data set"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Run Cross-validation with function (distantly supervised) "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extract text from tweets\n",
      "aggressive_train =  [tweet.text for tweet in results_aggressive]\n",
      "non_aggressive_train = [tweet.text for tweet in results_non_aggressive]\n",
      "\n",
      "messages_train = aggressive_train + non_aggressive_train\n",
      "\n",
      "categories_train = ['aggressive'] * len(aggressive_train) \\\n",
      "                 + ['non_aggressive'] * len(non_aggressive_train)\n",
      "\n",
      "distant_frame = pd.DataFrame()\n",
      "distant_frame['message'] = messages_train\n",
      "distant_frame['category'] = categories_train\n",
      "\n",
      "for i in xrange(0,7):\n",
      "    # print \"i is\", i\n",
      "    # Pay attention to add_selection_to_stopwords!\n",
      "    predicted_per_fold, target_per_fold = \\\n",
      "    leave_one_out_evaluation(frame\n",
      "                           , distant_frame\n",
      "                           , pipe = i\n",
      "                           , add_selection_to_stopwords = False)\n",
      "    evaluation(predicted_per_fold, target_per_fold)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Validation: Distant Supervision -> Manually Labeled"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Interactive test for getting a feeling for the accuracy (using the manually labeled data)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "interactive_test = [raw_input('Please type message that should be categorized: ')]\n",
      "\n",
      "train = frame.ix[[create_key(y) for y in range(0,10)]]\n",
      "    \n",
      "messages_train = train['message']\n",
      "categories_train = train['category']\n",
      "    \n",
      "# train classifier \n",
      "pipeline = pipeline.fit(messages_train, categories_train)\n",
      "\n",
      "predicted = pipeline.predict(interactive_test)\n",
      "#print(predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Inspect Bag of Words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "aggressive_train =  [tweet.text for tweet in results_aggressive]\n",
      "\n",
      "non_aggressive_train = [tweet.text for tweet in results_non_aggressive]\n",
      "\n",
      "# messages_train = aggressive_train + non_aggressive_train\n",
      "\n",
      "categories_train = ['aggressive'] * len(aggressive_train) \\\n",
      "                 + ['non_aggressive'] * len(non_aggressive_train)\n",
      "\n",
      "matrix = count_vect.fit_transform(aggressive_train, ['aggressive'] * len(aggressive_train))\n",
      "freqs = [(word, matrix.getcol(idx).sum()) for word, idx in count_vect.vocabulary_.items()]\n",
      "#sort from largest to smallest\n",
      "# print sorted (freqs, key = lambda x: -x[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Pickle"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Getting back the objects:\n",
      "with open('results_854.pickle') as f:\n",
      "    results_aggressive, results_non_aggressive = pickle.load(f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Saving the objects:\n",
      "with open('final_evaluation.pickle', 'w') as f:\n",
      "    pickle.dump([results_aggressive, results_non_aggressive], f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}